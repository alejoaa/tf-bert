{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ExTRI Named Entity Recognition Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes the gold-standard and validation sets from ExTRI and process them to create a Named Entity Regonition (NER) dataset. The code consists of the following steps:\n",
    "1. Import and preprocess the table of sentences from the _sentence coverage_ supplementary file from the ExTRI paper.\n",
    "2. Import and preprocess the table of gold-standard sentences from ExTRI.\n",
    "3. Import and preprocess the table of validation set sentences from ExTRI.\n",
    "4. Merge the gold-standard and validation set tables.\n",
    "5. Create a dictionary containing gene names and gene synonyms for each gene in the merged table.\n",
    "6. Create a Hugging Face NER dataset by annotating each sentence with a fine-tuned BERT model for the identification of genetic entities, then fuzzy matching those entities to the reported transcription factor(s) and target gene(s) to determine their classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing packages and setting global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "from itertools import islice\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from datasets import ClassLabel, Dataset, Features, Sequence, Value\n",
    "from pyhere import here\n",
    "from spacy import displacy\n",
    "from thefuzz import process\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File relative pathnames\n",
    "sentence_coverage_path = str(here(\"data/extri/interim/sentence_coverage.tsv\"))\n",
    "gold_standard_set_path = str(here(\"data/extri/interim/gold_standatrd_set.tsv\"))\n",
    "validation_set_path = str(here(\"data/extri/interim/validation_set.tsv\"))\n",
    "\n",
    "output_dir = str(here(\"data/extri/processed\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`construct_sentence_uid` is used for creating sentences unique IDs. In ExTRI, sentences are identified by their PMID and a number ID assigned within each PMID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sentence_uid(row):\n",
    "    # Builds a sentence unique id (uid) from a string that contains\n",
    "    # `PMID:Sentence ID:TF:TG`. It returns a string like `PMID:Sentence UID`\n",
    "\n",
    "    values = row[\"PMID:Sentence ID:TF:TG\"].split(\":\")\n",
    "    return f'{values[0]}:{values[1]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entities_for_displacy(dataset):\n",
    "    \"\"\"\n",
    "    This function takes as input a Hugging Face Dataset and\n",
    "    creates a list of dictionaries that contains each sentence's\n",
    "    entities in the format accepted by displacy.\n",
    "\n",
    "    To render a sentence with displacy any dictionary in the list\n",
    "    can be passed as input.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_entities = []\n",
    "\n",
    "    for row in tqdm(dataset):\n",
    "        \n",
    "        # Initialize values for parsing an entity\n",
    "        ents = []\n",
    "        parsing_entity = False\n",
    "        entity_start = None\n",
    "        entity_end = None\n",
    "        entity_label = None\n",
    "\n",
    "        tokens_list = row[\"tokens\"]\n",
    "        labels_list = row[\"ner_tags\"]\n",
    "        \n",
    "        # Get the text by joining the tokens\n",
    "        # do not get the text from the original sentence as some\n",
    "        # compound words will not have spaces between them\n",
    "        # therefore disrupting the matching of labels and tokens\n",
    "        text = \" \".join(tokens_list)\n",
    "\n",
    "        # Create a list of token character lengths\n",
    "        tokens_length = [len(string) for string in tokens_list]\n",
    "\n",
    "        for index, (token, label) in enumerate(zip(tokens_list, labels_list)):\n",
    "\n",
    "            label = features_schema[\"ner_tags\"].feature.int2str(label)\n",
    "            # Remove \"I-\" or \"B-\" from the label\n",
    "            label = re.sub(r\"^(B-|I-)\", \"\", label)\n",
    "\n",
    "            # If label contains an entity\n",
    "            if label != \"O\":\n",
    "\n",
    "                # If currently parsing an entity\n",
    "                if parsing_entity == True:\n",
    "\n",
    "                    # Update entity end\n",
    "                    entity_end = sum(tokens_length[:index + 1]) + index\n",
    "\n",
    "                # Start tracking an entity\n",
    "                else:\n",
    "                    parsing_entity = True\n",
    "                    entity_label = label\n",
    "                    entity_start = sum(tokens_length[:index]) + index # accounts for lenths of prev tokens plus spaces\n",
    "                    entity_end = entity_start + tokens_length[index]\n",
    "            \n",
    "            # If label is \"O\"\n",
    "            elif label == \"O\" or index == len(labels_list):\n",
    "\n",
    "                # If an entity was being parsed\n",
    "                if parsing_entity == True:\n",
    "                    entity_dict = {\n",
    "                        \"start\": entity_start,\n",
    "                        \"end\": entity_end,\n",
    "                        \"label\": entity_label\n",
    "                    }\n",
    "\n",
    "                    # Append to `ents`\n",
    "                    ents.append(entity_dict)\n",
    "\n",
    "                    # Reset values\n",
    "                    parsing_entity = False\n",
    "                    entity_start = None\n",
    "                    entity_end = None\n",
    "                    entity_label = None\n",
    "\n",
    "                # Not parsing an entity, continue to next token\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        # Create sentence object containing text and entities\n",
    "        sentence_entities_dict = {\n",
    "            \"text\": text,\n",
    "            \"ents\": ents\n",
    "        }\n",
    "\n",
    "        # Append sentence entities to output\n",
    "        dataset_entities.append(sentence_entities_dict)\n",
    "    \n",
    "    # Return sentences with entities  formatted for displacy\n",
    "    return dataset_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the sentences coverage table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `sentence coverage` table from ExTRI, which contains the following columns:\n",
    "- `PMID:Sentence ID:TF:TG`\n",
    "- `Transcription Factor (Associated Gene Name)`\n",
    "- `Target Gene (Associated Gene Name)`\n",
    "- `Sentence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sentence coverage table\n",
    "sentence_cov = pd.read_csv(\n",
    "    sentence_coverage_path,\n",
    "    sep=\"\\t\",\n",
    "    usecols=[\"PMID:Sentence ID:TF:TG\", \"Transcription Factor (Associated Gene Name)\", \"Target Gene (Associated Gene Name)\", \"Sentence\"]\n",
    ")\n",
    "\n",
    "# Rename columns\n",
    "sentence_cov.rename(columns={\n",
    "    \"Transcription Factor (Associated Gene Name)\": \"TF\",\n",
    "    \"Target Gene (Associated Gene Name)\": \"TG\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Create a column of unique ID (uid) for each sentence by merging the PMID and the sentence ID\n",
    "# This is because sentence IDs are created for each PMID\n",
    "sentence_cov[\"sentence_uid\"] = sentence_cov.apply(construct_sentence_uid, axis=1)\n",
    "\n",
    "# Keep only the `sentence_uid` column and drop duplicates\n",
    "sentence_cov = sentence_cov[[\"sentence_uid\", \"Sentence\"]].drop_duplicates()\n",
    "sentence_cov.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Gold-Standard set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the _gold-standard_ table from ExTRI and format it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_set = pd.read_csv(\n",
    "    gold_standard_set_path,\n",
    "    sep=\"\\t\",\n",
    "    encoding=\"latin\",\n",
    "    dtype={\"PMID\": \"Int64\", \"SID\": \"Int64\"}\n",
    ")\n",
    "\n",
    "# Create a column to indicate dataset source\n",
    "gs_set[\"dataset_source\"] = \"gold-standard\"\n",
    "\n",
    "# Create the sentence_uid column\n",
    "gs_set[\"sentence_uid\"] = gs_set.apply(lambda row: \"{}:{}\".format(row[\"PMID\"], row[\"SID\"]), axis=1)\n",
    "\n",
    "# Filter out sentences with comments or negated\n",
    "gs_set = gs_set[pd.isna(gs_set[\"Negated\"])]\n",
    "gs_set = gs_set[pd.isna(gs_set[\"Comments\"])]\n",
    "\n",
    "# Drop columns\n",
    "gs_set = gs_set.drop([\"PMID\", \"SID\", \"Negated\", \"Comments\"], axis=1)\n",
    "\n",
    "# Rename columns\n",
    "gs_set.rename(columns={\"DbTF\": \"TF\", \"Sentence\": \"sentence\"}, inplace=True)\n",
    "\n",
    "# Reorder columns\n",
    "gs_set = gs_set.loc[:, [\"sentence_uid\", \"sentence\", \"dataset_source\", \"TF\", \"TG\"]]\n",
    "\n",
    "gs_set.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a column of `PMID:Sentence ID` from this dataset to obtain the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = pd.read_csv(\n",
    "    validation_set_path,\n",
    "    sep=\"\\t\"\n",
    ")\n",
    "\n",
    "# Filter out invalid sentences\n",
    "validation_set = validation_set[validation_set[\"Valid\"] == \"Valid\"]\n",
    "\n",
    "# Create `sentence_uid` column\n",
    "validation_set[\"sentence_uid\"] = validation_set.apply(construct_sentence_uid, axis=1)\n",
    "\n",
    "# Add dataset source column\n",
    "validation_set[\"dataset_source\"] = \"Validation\"\n",
    "\n",
    "# Create TF and TG columns\n",
    "validation_set[\"TF\"] = validation_set.apply(lambda row: row[\"PMID:Sentence ID:TF:TG\"].split(\":\")[2], axis=1)\n",
    "validation_set[\"TG\"] = validation_set.apply(lambda row: row[\"PMID:Sentence ID:TF:TG\"].split(\":\")[3], axis=1)\n",
    "\n",
    "# Merge with `sentence_cov` to obtain sentences\n",
    "validation_set = pd.merge(validation_set, sentence_cov, how=\"left\", on=\"sentence_uid\")\n",
    "\n",
    "# Drop columns\n",
    "validation_set = validation_set.drop([\"PMID:Sentence ID:TF:TG\", \"Valid\"], axis=1)\n",
    "\n",
    "validation_set.rename(columns={\"Sentence\": \"sentence\"}, inplace=True)\n",
    "\n",
    "# Reorder columns\n",
    "validation_set = validation_set.loc[:, [\"sentence_uid\", \"sentence\", \"dataset_source\", \"TF\", \"TG\"]]\n",
    "\n",
    "validation_set.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge `sentences` dataframe with `validation_set` to add the `Sentence` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create DataFrame for combined sets (Gold-Standard and Validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `combined_sets_df` by concatenating the gold-standard and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sets_df = pd.concat([gs_set, validation_set], axis=0)\n",
    "print(f\"combined_sets_df shape is {combined_sets_df.shape}\")\n",
    "combined_sets_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Obtain gene synonyms for TFs and TGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a list of all genes (TFs and TGs) into `all_genes_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genes_list = list(pd.concat([combined_sets_df[\"TF\"], combined_sets_df[\"TG\"]]).unique())\n",
    "print(f\"Number of unique genes: {len(all_genes_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate rows in `combined_sets_df` with the same `sentence_uid` and concatenate TFs and TGs for the same sentence with a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sets_df = combined_sets_df.groupby([\"sentence_uid\", \"dataset_source\", \"sentence\"], as_index=False).agg(\n",
    "    {\n",
    "        \"TF\": lambda x: \",\".join(set(x)),\n",
    "        \"TG\": lambda x: \",\".join(set(x))\n",
    "    }\n",
    ")\n",
    "\n",
    "combined_sets_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a dictionary of genes and their synonyms using NCBI datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import subprocess\n",
    "\n",
    "genes_synonyms_dict = {}\n",
    "genes_without_entries = []\n",
    "\n",
    "for gene in tqdm(all_genes_list):\n",
    "    command = f\"datasets summary gene symbol {gene} --ortholog human,10090,10116 --as-json-lines | dataformat tsv gene --fields gene-id,symbol,tax-name,common-name,synonyms,gene-type,ensembl-geneids\"\n",
    "    result = subprocess.run(command, stdout=subprocess.PIPE, shell=True)\n",
    "    data = StringIO(result.stdout.decode(\"utf-8\"))\n",
    "\n",
    "    try:\n",
    "        ncbi_genes_df = pd.read_csv(data, sep=\"\\t\", na_values=\"NaN\")\n",
    "    except:\n",
    "        genes_without_entries.append(gene)\n",
    "        continue\n",
    "    \n",
    "    genes_synonyms_dict[gene] = []\n",
    "\n",
    "    # TODO: Simplify this loop.\n",
    "    for column_string in ncbi_genes_df[\"Synonyms\"]:\n",
    "\n",
    "        if pd.notna(column_string):\n",
    "            genes_synonyms_dict[gene].extend(column_string.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genes that were not present in NCBI datasets were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_without_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create NER Hugging Face dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create an empty `Dataset` (`ner_dataset`) and annotate the senteces from `combined_sets_df` with NER tags. First, we initialize the model and tokenizers. The `spacy_tokenizer` is used to tokenize the senteces, as they will be stored in the dataset (`ner_dataset`) in the form of a list of tokens. The `bert_tokenizer` is a Hugging Face tokenizer that will process each sentence for being input into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of NER tags\n",
    "entities_list = [\"O\", \"B-TRANSCRIPTION_FACTOR\", \"I-TRANSCRIPTION_FACTOR\", \"B-TARGET_GENE\", \"I-TARGET_GENE\"]\n",
    "\n",
    "# Initialize tokenizers and NER model\n",
    "spacy_tokenizer = get_tokenizer(\"spacy\", language = \"en_core_web_trf\")\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"alvaroalon2/biobert_genetic_ner\")\n",
    "\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"alvaroalon2/biobert_genetic_ner\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Initialize Dataset\n",
    "# ner_dataset = Dataset(\n",
    "#     pa.table({\n",
    "#         \"sentence_uid\": [],\n",
    "#         \"dataset_source\": [],\n",
    "#         \"sentence\": [],\n",
    "#         \"tokens\": [],\n",
    "#         \"ner_tags\": []\n",
    "#     })\n",
    "# )\n",
    "\n",
    "# Features schema\n",
    "features_schema = Features({\n",
    "    \"sentence_uid\": Value(dtype=\"string\"),\n",
    "    \"dataset_source\": Value(dtype=\"string\"),\n",
    "    \"sentence\": Value(dtype=\"string\"),\n",
    "    \"tokens\": Sequence(feature=Value(dtype=\"string\")),\n",
    "    \"ner_tags\": Sequence(\n",
    "        ClassLabel(\n",
    "            num_classes= len(entities_list),\n",
    "            names=entities_list\n",
    "        )\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iterate over each row in `combined_sets_df` and tag entities in each sentence as TF or TG using a Hugging Face NER model and fuzzy search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each row in `combined_sets_df`\n",
    "dataset_entries = []\n",
    "for _, row in tqdm(combined_sets_df.iterrows(), total=combined_sets_df.shape[0]):\n",
    "\n",
    "    # Remove repeated whitespaces. This would cause an error later since\n",
    "    # word_ids from the BERT tokenizer removes whitespaces\n",
    "    sentence = re.sub(\" +\", \" \", row[\"sentence\"])\n",
    "    # Tokenize sentence with spacy\n",
    "    tokenized_text = spacy_tokenizer(sentence)\n",
    "\n",
    "    # Tokenize with HF tokenizer\n",
    "    tokenized_input = bert_tokenizer(tokenized_text, padding=False, truncation=True, max_length=512, is_split_into_words=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Move tensors to device\n",
    "    tokenized_input = tokenized_input.to(device)\n",
    "\n",
    "    results = model(**tokenized_input)\n",
    "    logits = results[\"logits\"].squeeze(0)\n",
    "    _, label_preds = torch.max(logits, 1)\n",
    "\n",
    "    \"\"\"\n",
    "    Iterate over each index of (BERT-)tokenized words to assign a label (model prediction)\n",
    "    to each word in the original sentence.\n",
    "    \"\"\"\n",
    "    sentence_ner_tags = []\n",
    "    prev_word_idx = None\n",
    "\n",
    "    word_ids = tokenized_input.word_ids(batch_index=0)\n",
    "    # `word_idx` is the corresponding index to the word in the spacy tokenized text\n",
    "    for enum_idx, word_idx in enumerate(word_ids):\n",
    "\n",
    "        if (word_idx is None) or (word_idx == prev_word_idx):\n",
    "            continue\n",
    "\n",
    "        # New word (token)\n",
    "        else:\n",
    "\n",
    "            # If label is different than 'O' (it contains a genetic entity)\n",
    "            if label_preds[enum_idx] != 2:\n",
    "        \n",
    "                # Check if previous NER tag was a genetic entity\n",
    "                # If so, add a I-GENTIC tag\n",
    "                if len(sentence_ner_tags) > 0 and (sentence_ner_tags[-1] == \"B-GENETIC\" or sentence_ner_tags[-1] == \"I-GENETIC\"):\n",
    "                    sentence_ner_tags.append(\"I-GENETIC\")\n",
    "                else:\n",
    "                    sentence_ner_tags.append(\"B-GENETIC\")\n",
    "\n",
    "            # If label is 'O'\n",
    "            else:\n",
    "                sentence_ner_tags.append(\"O\")\n",
    "\n",
    "            # Update `prev_word_idx`\n",
    "            prev_word_idx = word_idx\n",
    "\n",
    "    # Get TFs and TGs and their synonyms\n",
    "    tfs = []\n",
    "    tgs = []\n",
    "\n",
    "    # TODO: This code can be improved. It is repeated for TF and TG.\n",
    "    for tf in row[\"TF\"].split(\",\"):\n",
    "        if tf in genes_synonyms_dict.keys():\n",
    "            tfs.extend(list(set(genes_synonyms_dict.get(tf) + [tf])))\n",
    "        else:\n",
    "            tfs.append(tf)\n",
    "    \n",
    "    for tg in row[\"TG\"].split(\",\"):\n",
    "        if tg in genes_synonyms_dict.keys():\n",
    "            tgs.extend(list(set(genes_synonyms_dict.get(tg) + [tg])))\n",
    "        else:\n",
    "            tgs.append(tg)\n",
    "\n",
    "    current_entity_indexes = []\n",
    "    num_tf_entities = 0\n",
    "    num_tg_entities = 0\n",
    "\n",
    "    # Iterate over each word token (from spacy tokenization)\n",
    "    for enum_idx, token_idx in enumerate(tokenized_text):\n",
    "\n",
    "        # Get label for current token (either B-GENETIC, I-GENETIC, or O)\n",
    "        try:\n",
    "            current_label = sentence_ner_tags[enum_idx]\n",
    "        except:\n",
    "            print(f\"Length of tokenized_text: {len(tokenized_text)}\")\n",
    "            print(f\"Length of sentence_ner_tags: {len(sentence_ner_tags)}\")\n",
    "            raise Exception(f\"Error for sentence {tokenized_text} |TAGS:| {sentence_ner_tags}\")\n",
    "        \n",
    "        # If word is part of a genetic entity then add its index to `current_entity_indexes`\n",
    "        if current_label == \"B-GENETIC\" or current_label == \"I-GENETIC\":\n",
    "            current_entity_indexes.append(enum_idx)\n",
    "\n",
    "        # If word is not part of a genetic entity or it is the last word in the sentence\n",
    "        if (current_label == \"O\") or (enum_idx == len(tokenized_text) - 1):\n",
    "\n",
    "            # If an entity is being parsed from previous token(s) or last token\n",
    "            if len(current_entity_indexes) > 0:\n",
    "                genetic_entity = \" \".join([tokenized_text[index] for index in current_entity_indexes])\n",
    "\n",
    "                # Get score for TF and TG based on fuzzy search\n",
    "                _, tf_match_score = process.extractOne(genetic_entity, tfs)\n",
    "                _, tg_match_score = process.extractOne(genetic_entity, tgs)\n",
    "\n",
    "                # If match score is lower than 50 it likely doesn't belong to any TF or TG.\n",
    "                # Then set label to \"O\"\n",
    "                if tf_match_score < 50 and tg_match_score < 50:\n",
    "                    entity_label = \"O\"\n",
    "\n",
    "                    # Update corresponding indexes in sentence_ner_tags to \"O\"\n",
    "                    for entity_index in current_entity_indexes:\n",
    "                        sentence_ner_tags[entity_index] = entity_label\n",
    "                else:\n",
    "\n",
    "                    if tf_match_score > tg_match_score:\n",
    "                        entity_label = \"TRANSCRIPTION_FACTOR\"\n",
    "                        num_tf_entities += 1\n",
    "                    else:\n",
    "                        entity_label = \"TARGET_GENE\"\n",
    "                        num_tg_entities += 1\n",
    "                    \n",
    "                    # Update corresponding indexes in sentence_ner_tags\n",
    "                    for entity_index in current_entity_indexes:\n",
    "                        sentence_ner_tags[entity_index] = sentence_ner_tags[entity_index].replace(\"GENETIC\", entity_label)\n",
    "\n",
    "                # Reset `current_entity_indexes`\n",
    "                current_entity_indexes = []\n",
    "    \n",
    "    try:\n",
    "        # sentence_ner_tags = [features_schema[\"ner_tags\"].feature.str2int(tag) for tag in sentence_ner_tags]\n",
    "        [features_schema[\"ner_tags\"].feature.str2int(tag) for tag in sentence_ner_tags]\n",
    "    except:\n",
    "        raise Exception(f\"{row['sentence']} |TAGS| {sentence_ner_tags}\")\n",
    "\n",
    "    # Add sentence to dataset if it contains at least one TF and TG entities \n",
    "    if num_tf_entities >= 1 and num_tg_entities >= 1:\n",
    "\n",
    "        dataset_entry = {\n",
    "            \"sentence_uid\": row[\"sentence_uid\"],\n",
    "            \"dataset_source\": row[\"dataset_source\"],\n",
    "            \"sentence\": row[\"sentence\"],\n",
    "            \"tokens\": tokenized_text,\n",
    "            \"ner_tags\": sentence_ner_tags\n",
    "        }\n",
    "\n",
    "        # ner_dataset = ner_dataset.add_item(dataset_entry)\n",
    "        dataset_entries.append(dataset_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset = Dataset.from_list(dataset_entries, features=features_schema)\n",
    "ner_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the entities in the dataset using `displacy`, we run our custom function that formats the Hugging Face dataset for input to `displacy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset_displacy_entities = create_entities_for_displacy(ner_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = ner_dataset_displacy_entities[0]\n",
    "\n",
    "options = {'colors': {'TRANSCRIPTION_FACTOR': '#D6D77F', 'TARGET_GENE': '#01DFED'}}\n",
    "displacy.render(example_sentence, manual=True, style=\"ent\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the splits for the dataset (train and validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset_dict = ner_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Rename test to validation\n",
    "validation_set = ner_dataset_dict.pop(\"test\")\n",
    "ner_dataset_dict[\"validation\"] = validation_set\n",
    "ner_dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset_dict.save_to_disk(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the dataset run:\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    \"train\": str(here(os.path.join(output_dir, \"train/data-00000-of-00001.arrow\"))),\n",
    "    \"validation\": str(here(os.path.join(output_dir, \"validation/data-00000-of-00001.arrow\")))\n",
    "}\n",
    "\n",
    "raw_dataset = load_dataset(\"arrow\", data_files=data_files)\n",
    "raw_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
