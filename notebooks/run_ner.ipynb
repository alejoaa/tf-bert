{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tfBERT: Finetuned BERT model for NER of transcription factors and target genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this file finetunes a BERT model for Named Entity Recognition. Jupyter cells with the _remove_cell_ tag will be ignored when using the custom Jinja template included in this project for `nbconvert`, used to convert the Python notebook to a Python script file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The impots Jupyter cell has a tag of _imports_, which is important for the Jinja template as it will add the `argparse` package below such cell code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from datasets import ClassLabel, load_dataset, Sequence\n",
    "from pyhere import here  # type: ignore\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup variables and create logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define torch device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if device == torch.device(\"mps\"):\n",
    "    torch.mps.manual_seed(seed)\n",
    "elif device == torch.device(\"cuda\"):\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logger\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: str\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    \n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "    text_column_name: str\n",
    "    label_column_name: str\n",
    "    max_seq_length: int\n",
    "    \n",
    "    # split_text_into_tokens: bool # will use .split() for text and labels column # TO REMOVE\n",
    "\n",
    "    dataset_name: Optional[str] = field(default=None)\n",
    "    load_script: Optional[str] = field(default=None)\n",
    "\n",
    "    train_file: Optional[str] = field(default=None)\n",
    "    validation_file: Optional[str] = field(default=None)\n",
    "    test_file: Optional[str] = field(default=None)\n",
    "\n",
    "    max_train_samples: Optional[int] = field(default=None)\n",
    "    max_eval_samples: Optional[int] = field(default=None)\n",
    "\n",
    "    label_all_tokens: bool = field(default=False)\n",
    "\n",
    "    return_entity_level_metrics: bool = field(default=False)\n",
    "\n",
    "    metric: str = field(default=\"seqeval\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all texts and align the labels with them\n",
    "def tokenize_and_align_labels(examples):\n",
    "    \n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[data_args.text_column_name],\n",
    "        truncation = True,\n",
    "        max_length = data_args.max_seq_length,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[data_args.label_column_name]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index = i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_to_id[label[word_idx]])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the data_args.label_all_tokens flag\n",
    "            else:\n",
    "                if data_args.label_all_tokens:\n",
    "                    label_ids.append(b_to_i_label[label_to_id[label[word_idx]]])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_list(labels):\n",
    "    unique_labels = set()\n",
    "    for label in labels:\n",
    "        unique_labels = unique_labels | set(label)\n",
    "    label_list = list(unique_labels)\n",
    "    label_list.sort()\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis = 2)\n",
    "\n",
    "    # Remove  ignored idex (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    if data_args.return_entity_level_metrics:\n",
    "        # Unpack nested dictionaries\n",
    "        final_results = {}\n",
    "        for key, value in results.items():\n",
    "            if isinstance(value, dict):\n",
    "                for n, v in value.items():\n",
    "                    final_results[f\"{key}_{n}\"] = v\n",
    "            else:\n",
    "                final_results[key] = value\n",
    "        return final_results\n",
    "    else:\n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# This Jupyter cell will be removed when the notebook is converted to a Python script\n",
    "config_file = here(\"configs/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_yaml_file(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format = \"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt = \"%m/%d/%Y %H:%M/%S\",\n",
    "    handlers = [logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "if training_args.should_log:\n",
    "    # The default of training_args.log_level is passive (defaults to warning), so we set log level at info here to have that default\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n",
    "    + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the dataset either form Hugging Face or local files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"### Loading the dataset ###\")\n",
    "\n",
    "# If dataset name from Hugging Face is provided\n",
    "if data_args.dataset_name is not None:\n",
    "    raw_datasets = load_dataset(data_args.dataset_name, cache_dir=model_args.cache_dir)\n",
    "\n",
    "# Otherwise, import from local files\n",
    "else:\n",
    "    data_files = {}\n",
    "    if data_args.train_file:\n",
    "        data_files[\"train\"] = data_args.train_file\n",
    "    if data_args.validation_file:\n",
    "        data_files[\"validation\"] = data_args.validation_file\n",
    "    if data_args.test_file:\n",
    "        data_files[\"test\"] = data_args.test_file\n",
    "\n",
    "    if data_args.load_script == \"csv\":\n",
    "        raw_datasets = load_dataset(\"csv\", data_files=data_files, sep=\"\\t\")\n",
    "    elif data_args.load_script == \"arrow\":\n",
    "        raw_datasets = load_dataset(\"arrow\", data_files=data_files)\n",
    "\n",
    "logger.info(f\"Loaded dataset(s):\\n{raw_datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a label list for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = raw_datasets[\"train\"].column_names # type: ignore\n",
    "features = raw_datasets[\"train\"].features # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_are_int = isinstance(features[data_args.label_column_name].feature, ClassLabel)\n",
    "if labels_are_int:\n",
    "    label_list = features[data_args.label_column_name].feature.names\n",
    "    label_to_id = {i: i for i in range(len(label_list))}\n",
    "else:\n",
    "    label_list: [str] = get_label_list(raw_datasets[\"train\"][data_args.label_column_name]) # type: ignore\n",
    "    label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "\n",
    "num_labels = len(label_list)\n",
    "\n",
    "logger.info(f\"Number of labels in dataset: {num_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained model and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model and move it to the device (e.g. GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"### Loading pretrained model and tokenizer ###\")\n",
    "\n",
    "# Load model with a dropout probability for the classifier head\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_args.model_name,\n",
    "    num_labels = num_labels,\n",
    "    classifier_dropout = 0.1,\n",
    "    cache_dir=model_args.cache_dir\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "if training_args.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_args.model_name, cache_dir=model_args.cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the correspondences label/ID inside the model config\n",
    "model.config.label2id = {l: i for i, l in enumerate(label_list)}\n",
    "model.config.id2label = dict(enumerate(label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an alternative to `label_list` were \"B-\" tags are replaced by their corresponding \"I-\" tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map that sends B-Xxx label to its I-Xxx counterpart\n",
    "b_to_i_label = []\n",
    "for idx, label in enumerate(label_list):\n",
    "    if label.startswith(\"B-\") and (label.replace(\"B-\", \"I-\") in label_list):\n",
    "        b_to_i_label.append(label_list.index(label.replace(\"B-\", \"I-\")))\n",
    "    else:\n",
    "        b_to_i_label.append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize all texts and align labels and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"### Pre-processing the dataset ###\")\n",
    "\n",
    "# Tokenize train dataset\n",
    "if training_args.do_train:\n",
    "    if \"train\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    train_dataset = raw_datasets[\"train\"]\n",
    "    if data_args.max_train_samples is not None:\n",
    "        max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "        train_dataset = train_dataset.select(range(max_train_samples))\n",
    "    train_dataset = train_dataset.map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        desc=\"Running tokenizer on train dataset\"\n",
    "    )\n",
    "\n",
    "# Tokenize validation dataset\n",
    "if training_args.do_eval:\n",
    "    if \"validation\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_eval requieres a validation dataset\")\n",
    "    eval_dataset = raw_datasets[\"validation\"]\n",
    "    if data_args.max_eval_samples is not None:\n",
    "        max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
    "        eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        desc=\"Runing tokenizer on validation dataset\"\n",
    "    )\n",
    "\n",
    "# TODO: Do the same for do_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the data collator and metrics to be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True, pad_to_multiple_of=None)\n",
    "\n",
    "# Metrics\n",
    "metric = evaluate.load(data_args.metric, cache_dir=model_args.cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Hugging Face `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # callbacks = [EarlyStoppingCallback(early_stopping_patience=6)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the argument `do_train` is set to `True` in the user's config file, then training is performed. First, we look for training checkpoints in the output directory. If there are none, then training starts anew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_args.do_train:\n",
    "    logger.info(\"### Training ###\")\n",
    "    last_checkpoint: Optional[str] = None\n",
    "    if os.path.isdir(training_args.output_dir) and not training_args.overwrite_output_dir:\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "\n",
    "    train_result = trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "\n",
    "    # Obtain metrics\n",
    "    metrics = train_result.metrics\n",
    "\n",
    "    max_train_samples = (\n",
    "        data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
    "    )\n",
    "    \n",
    "    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the argument `do_eval` is set to `True` in the user's config file, then evaluation is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_args.do_eval:\n",
    "    logger.info(\"### Evaluation ###\")\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "\n",
    "    max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n",
    "    metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "    \n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save metrics and do the same for do_predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
