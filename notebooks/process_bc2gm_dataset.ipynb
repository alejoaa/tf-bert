{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format BC2GM dataset to tsv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code formats the BC2GM dataset to a tsv format and also exports its in Arrow format. Paths are relative to the root project as this code uses `pyhere`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pyhere import here\n",
    "from datasets import ClassLabel, Dataset, Features, Value, Sequence, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input filenames should be the name of the split they correspond to (e.g. _train_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input files\n",
    "input_files = [\n",
    "    \"data/bc2gm/raw/train.tsv\",\n",
    "    \"data/bc2gm/raw/test.tsv\",\n",
    "    \"data/bc2gm/raw/devel.tsv\",\n",
    "\n",
    "]\n",
    "\n",
    "# Define output folder\n",
    "output_dir = \"data/bc2gm/processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize `DatasetDict` and `Features` schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_list = [\"O\", \"B-GENE\", \"I-GENE\"]\n",
    "\n",
    "# Initialize DatasetDict\n",
    "ner_dataset = DatasetDict()\n",
    "\n",
    "# Features schema\n",
    "features_schema = Features({\n",
    "    \"sentence\": Value(dtype=\"string\"),\n",
    "    \"tokens\": Sequence(feature=Value(dtype=\"string\")),\n",
    "    \"ner_tags\": Sequence(\n",
    "        ClassLabel(\n",
    "            num_classes=len(entities_list),\n",
    "            names=entities_list\n",
    "        )\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the all input files\n",
    "for file_name in input_files:\n",
    "\n",
    "    print(f\"Processing {file_name}\")\n",
    "\n",
    "    # Read lines of input file\n",
    "    with open(here(file_name), \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Initialize values\n",
    "    dataset_entries = []\n",
    "    sentence_words: List[str] = []\n",
    "    label_items: List[str] = []\n",
    "\n",
    "    # Iterate over each line\n",
    "    for idx, line in tqdm(enumerate(lines), total=len(lines)):\n",
    "\n",
    "        # If line is not new line character (it contains a word and a label)\n",
    "        if line != \"\\n\":\n",
    "            \n",
    "            # Split line to get word and label\n",
    "            line_elements = line.strip().split(\"\\t\")\n",
    "            word = line_elements[0]\n",
    "            label = line_elements[1]\n",
    "            \n",
    "            # Append word and label\n",
    "            sentence_words.append(word)\n",
    "            label_items.append(label)\n",
    "\n",
    "        # If line is a new line character\n",
    "        else:\n",
    "            # Create sentence string by joining words list\n",
    "            sentence_string = \" \".join(sentence_words)\n",
    "\n",
    "            # Add entry to ner_dataset\n",
    "            dataset_entry = {\n",
    "                \"sentence\": sentence_string,\n",
    "                \"tokens\": sentence_words,\n",
    "                \"ner_tags\": label_items\n",
    "            }\n",
    "\n",
    "            # Append dataset entry to list of entries\n",
    "            dataset_entries.append(dataset_entry)\n",
    "\n",
    "            # Reset values\n",
    "            sentence_words = []\n",
    "            label_items = []\n",
    "    \n",
    "    # Create Dataset and append to DatasetDict\n",
    "    dataset_split_name = Path(file_name).stem\n",
    "    ner_dataset[dataset_split_name] = Dataset.from_list(dataset_entries, features=features_schema)\n",
    "\n",
    "    # Remove dataset_entries from memory\n",
    "    del dataset_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset.save_to_disk(str(here(output_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the dataset run:\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    \"train\": str(here(os.path.join(output_dir, \"train/data-00000-of-00001.arrow\"))),\n",
    "    \"test\": str(here(os.path.join(output_dir, \"test/data-00000-of-00001.arrow\"))),\n",
    "    \"devel\": str(here(os.path.join(output_dir, \"devel/data-00000-of-00001.arrow\")))\n",
    "}\n",
    "\n",
    "raw_dataset = load_dataset(\"arrow\", data_files=data_files)\n",
    "raw_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
